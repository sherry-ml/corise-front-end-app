{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "The Enterprise LLM Landscape with Atul Deo - #640", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " I want to send a big thanks to our friends at Amazon Web Services for their continued support of the show and their sponsorship of this episode. You know AWS as a cloud computing technology leader, but did you realize the company offers a broad array of services and infrastructure at all three layers of the machine learning technology stack? AWS has helped more than 100,000 customers of all sizes and across industries to innovate using ML and AI with industry-leading capabilities. And they're taking the same approach to make it easy, practical, and cost-effective for customers to use generative AI in their businesses. At the bottom layer of the ML stack, they're making generative AI cost-efficient with Amazon EC2 Inf2 instances powered by AWS Inferentia 2 chips. At the middle layer, they're making generative AI app development easier with Amazon Bedrock, a managed service that makes pre-trained foundation models easily accessible via an API. And at the top layer, Amazon Code Whisperer is generally available now with support for more than 10 programming languages. To learn more about AWS ML and AI services and how they're helping customers accelerate their machine learning journeys, visit twimlai.com slash go slash AWS ML. All right, everyone. Welcome to another episode of the Twiml AI Podcast. I am your host, Sam Charrington. And today I'm joined by Atul Dayo. Atul is general manager of Amazon Bedrock. Before we get into today's conversation, be sure to hit that subscribe button wherever you're listening to today's show. Atul, welcome to the podcast. Thanks, Sam. Excited to be here today. I'm super excited to have you on the show and I'm looking forward to kind of diving in with you and talking through kind of this crazy LLM landscape, generative AI landscape that is rapidly evolving. But before we do that, I'd love to have you share a little bit about your background and your role at Amazon. Sure. So I lead product and engineering for Amazon Bedrock. I've been with Amazon for the last eight and a half years. I started my career as a software developer many years ago. I wrote code for a few years, then felt a need to understand business concepts, went to business school, and then worked for a few years in various functions such as corporate development. And then in the last few years, I've been busy building products at AWS. You know, one of the things that I've been kind of finding interesting about this point in time is that a lot of the more sophisticated enterprises had over the past several years gotten onto the machine learning train, built up competency around traditional models and everything that came along with that, MLOps. And now there's still an opportunity to get value out of AI, but it's a totally different language that we're asking them to use and think about. We're throwing out terms like LoRa and parameter-efficient tuning and RLHF. When I talk to folks, particularly folks that are at the executive level, they're overwhelmed with a lot of this language. And really, they, for better or for worse, mostly for worse, they want magic. I use this chat GPT thing. It seems like magic. I want that magic from my enterprise. Do you see similar things? I definitely hear a lot of this from customers. And I think customers want to understand or unravel what is really going on here. Because for them, they have some experience in machine learning at a smaller scale within their company. But this almost feels like a paradigm shift. And so let me just try to take a quick stab at demystifying some of that for the listeners. So in the past, customers or companies typically would embark upon building task-specific models for a particular use case. Let's just take a use case of detecting fraud in a particular organization or creating a chat bot for customer care. Now, what would typically happen in a large company is that a particular leader would go to an SVP or a VP and say, hey, this is an important problem for our company. And I need to build a machine learning model for this particular problem. And I need to go hire a few scientists. I need to go annotate a bunch of data to build a model for this. And here's my bill. And the VP would evaluate the proposal on a case-by-case basis and would basically say, you know, the problem looks interesting. It's strategic. The cost is decent. Let's go build a machine learning model. And a few weeks back, the team came back and said, you know, we build a model. It's working great. And the VP is happy. Now, the problem is this approach worked well for a few use cases. And the moment customers or companies saw that machine learning was effective for their organization, they wanted to scale it for kind of moving from tens of use cases to hundreds of use cases within their company. And also they were finding problems with doing that because first of all, machine learning experts in this world are still limited. It's 2023, but the problem, which was true in 2013, is still true in 2023. There are only so many data scientists and ML experts out there. And furthermore, the cost and the effort of doing custom annotation can be pretty significant. It's just a lot of effort. And it's not just about finding the humans to do it. You need the data scientists and the ML experts to again be involved in kind of defining what annotations need to happen. They need to kind of evaluate the quality of the annotations that come back and essentially ensure that a machine learning model is getting the right quality of data. Now, this process can be pretty onerous for a lot of small and medium sized businesses, let alone large companies. Right? So I think with foundation models, the big thing that has changed is that you're no longer training individual task specific models. There's almost like this centralized training of one large model that makes use of a large amount of unlabeled data. Now, I'm stressing on the word unlabeled data, because unlabeled data is easy to get, right? Unlike labeled data, it depends upon a lot of human effort. You can basically get a lot of data from the internet, from a bunch of different proprietary sources, and you can use them at scale without having to depend upon an army of people to go label the data. Basically saying a dog is a dog and a cat is a cat. Simply oversimplifying the labeling process, but doing that at scale, even for example, an image, right? Identifying every single part in a single image can be pretty onerous. So doing that has really freed up the ability for people to train this large language models. Now, what happens under the code is- You're almost making it sound easy though. It is. Actually, in some ways, the bottleneck or the challenge is, you know, you have to collect a large amount of unlabeled data. You do have to apply hygiene on using this data for training. Then there is, you need a large amount of compute to make use of this data. And now the way it happens is you throw all this compute and the model makes use of algorithms to learn the relationships between the words on its own. You're not really teaching it anything in terms of labeled data. The model understands sequential relationship between the words and then says, you know, I understand basically that, Hey, if there are a set of words before this, I roughly know what is the next word that's going to follow based on, again, I'm simplifying for our illustration purposes. But at the heart of it, that's basically what is happening. And every time the model generates next kind of output, it uses that as the input to predict the next set of outputs. So it's basically called as the autoregressive behavior. Now, at the end of all this, what you get is a well-trained model, which has completed its pre-training process. And it is capable of dealing with a large number of tasks out of the box. Now, the standard set of tasks for a text, a generative text model could be things like sentiment analysis, classification, information extraction, being able to converse, right? These are, again, the list goes on and on. There's a lot of tasks that come kind of automatically as part of the pre-training process. And now customers can take this model and adapt it for their particular use case within a company by just using a very small amount of labeled data. Now, this dramatically simplifies some of the things that we discussed earlier, which is companies had to go hire teams, invest in a lot of compute for training individual models. And now they just get a ready-made pre-trained model at somebody else. In this case, Amazon, we do pre-train our own models as part of Bedrock. And also we have partners like Anthropic, who have developed this large foundation models. But somebody else has done the work, the hard work for them, which is a great starting point, right? That's kind of the way to think about it. Companies get an incredible starting point that they can then take and either use it out of the box. In most cases, they can use it out of the box. In some cases, they may have to customize it, or there are different ways of customization, fine-tuning, continued pre-training, reinforcement learning with human feedback. Those are some different and Laura, like the parameter efficient fine-tuning. So that's kind of the general kind of structure, which is use it out of the box or customize it by changing the model's weights. Yeah, I kind of quip the, hey, you make it sound easy. And certainly in many ways, it's easier than you're certainly using a pre-trained model is easier than if you need to train it yourself. But there are still challenges associated with getting these models to do the productive things that you want them to do. I'm curious, what are the things that you find folks running into when they're trying to put these models to use? Absolutely. So that is a key question because think about the analogy. I'm going to start off with an analogy and then we will get into the exact specifics. So think about a company that hires an extremely smart person from the outside, takes them, hires them, but puts them in a conference room with no access to their internal systems or internal documents. And this employee is not going to be very productive. So a raw pre-trained LLM is almost like this employee who is sitting in a conference room with no access. So what needs to happen is this LLM or this employee in this analogy needs to be given access to different tools and it needs to be made cognizant of the different data sources in the company. That's when the LLM can be truly productive for the organization. And there are different ways of going about it. Now, one approach is to use the model in an out of the box capacity. And then the other approach is actually customizing that using the proprietary data. Now let's dive deep into each of those different kind of approaches. Now in the first one where you can use the model in out of the box capacity, there are, you know, again, for most listeners of your podcast, they may be familiar with the term called as prompt. That is the basic interaction model with which you communicate with large language models. Essentially it's a natural language command. The simplest example is, hey, here's a doc. Can you summarize it? And boom, you basically pass a doc and you get a summary as a result. And this simple way is called as zero short learning. Essentially you give a command, you get a response. Now the next kind of, you know, on the spectrum, the more complicated one is something called as in context learning, where you say, hey, I'm still dealing with out of the box. I'm not changing the model's weights in terms of customization, but I'm basically now saying, here are a few examples of how I want you to do this. Do it this way. And this is all part of the prompt. The model takes a look and basically says, I got it. Here's the answer. So that's in context learning. Now a few shot learning or a few short learning. Correct. Now in context learning is basically a very powerful tool because there are a lot of models out there that essentially have this concept of context length. How much data can you pass as part of your context window that allows you to do more and more powerful things. Now some of the models from Anthropic, for example, they recently announced that they are supporting a hundred thousand tokens as the context length. Now that is pretty compelling because which means that you can pass really long documents, really long set of examples, or you can just pass a lot of contextual metadata as part of the prompt and then get your answers without actually doing customization by tinkering with the model weights. Now let me be more specific. Now there's a technique called as retrieval augmented generation that has emerged in the last few weeks. Essentially this goes back to the point of giving a smart employee access to your company's documents. Now in some cases, I may want to get the model to actually do Q and A on my documents. Now the way to do that is I can pass some of the relevant pieces of the documents in my company as part of the prompt itself and basically then ask the model, hey, here are a bunch of documents. Here's my question. Can you go take a look at these documents and think through them and basically give me a coherent response. And now essentially within context learning or kind of RAG, essentially you get precisely that. RAG is retrieval augmented generation? Correct. I hear this as probably the number one kind of generative AI use case or requests. My experience in it's usually voiced as, hey, I want chat GPT buffer my data. Yeah. So Sam, the way companies do that typically is that let's just take the use case of being able to answer questions on top of company's knowledge base. Now to do this, there are several steps involved today. And obviously these steps are going to simplify over the next few weeks or months as more and more kind of companies, you know, they understand the customer pain points and simplify them. So the first step today is a company has a bunch of documents in their knowledge base. They essentially take these documents and then they chunk them into smaller pieces. So chunking is essentially converting them into small paragraphs or lines, relevant portions. And then essentially you pass those chunks into what we call as an embeddings model. Now an embeddings model is a type of a large language model that takes a text as input and returns embeddings or mathematical representations that capture the semantic meaning of the input text. Now, once you get these embeddings, you put them into a vector database. There are many popular vector databases out there, such as Pinecone, VV8 and open search. Now, once a company has gone through this process where they have converted all their internal documents, they've chunked them, they've created the embeddings and then they've stored them into a vector database. Let's just take a hypothetical user query. Like, hey, I want to find out a response to a question, which is what is the vacation policy for my company for a full-time employee? Now behind the scenes, what's going to happen is that user query is going to be passed into an embeddings model and you're going to get embeddings associated with that input query. And then under the hood, you basically do some matching between the embeddings for the user query and all the embeddings stored in the vector database. And essentially the relevant chunks of the text documents are extracted and then they are passed to the large language models as part of the context. And now when the large language model sees the original question, which is the query from the user, along with the necessary or the relevant pieces of documents, the large language model can then essentially formulate a very human-like answer that answers the precise question. Now again, these are all steps happening in the background and what the user just sees is a well-crafted answer that sounds very human-like. And it is a huge improvement over some of the traditional approaches that have existed in the past, like which made use of things like keyword-based search. So essentially with the large language models, both the generative model and the using the combination with the embeddings model, essentially we are reinventing a use case like semantic search. Now, just while we are on the topic of embeddings model, this model actually, which doesn't get a lot of love out there on Twitter and whatnot, but compared to the main generative model, but it's a very nifty model. It is used for many interesting use cases like clustering, anomaly detection, and things like that. But semantic search is really one use case where it shines and it's kind of made some headlines. When you describe this process, it sounds very straightforward. What are you seeing in terms of the challenges that folks run into trying to do this at scale? Like everybody wants this thing, right? Absolutely. They all want this, but not everyone has it yet. It must be harder than, hey, let's just throw a pine cone and suck all our documents in, and we'll have this capability. What are the big sticky points? So I think there are a couple of things. I think companies are still trying to figure out which models they use, which providers they use, and they also want to figure out whether they are going to build some of these applications in-house or whether they want to use application themselves. And also I think overall, I do think that companies don't necessarily want to think in terms of vector databases. They want to basically think in terms of here are my documents, here's my kind of, here's the solution to answer questions on top of my documents. They don't want to really deal with all the nitty gritties of this workflow that I described. I think on a related topic, which is we gave the example of that well-educated employee kind of that's hired without having access to tools and data sources. So we touched upon as part of this retrieval augmented generation workflow, we touched upon a lot of for the data sources, but there is also an interesting kind of trend evolving in the last few months, which is based on this paper called React, which is reasoning and action, which I think is one of the, I don't know if it got as much attention, but it did. I mean, obviously people have been using the concept behind it like crazy, but essentially what it does, it allows companies to make use of the LLM invoke APIs, and it relies a lot upon the model's reasoning capabilities. So the way to think about this is the idea behind tools or plugins that an LLM can use to take action in the context of the response. Now, just in terms of setup, what happens typically is some administrator will have to create an agent and basically say this agent's purpose is to do so and so, and it has to specify a list of resources. The resources could be a different set of APIs and what are the APIs supposed to do, what are the different parameters, and just plain English descriptions of what those parameters are supposed to do, and what kind of outputs does that function return or the API return, right? And then things like that. And similarly, they can also add some data sources. Now, what happens behind the scenes is ultimately you're still doing, at the end of it, you're doing a retrieval of the generation, because you're passing all that metadata as part of the prompt, and the model then kind of based on the user query, it figures out which of the APIs and which data sources should I use, can make multiple invocations of the model, and the way it works is I may go to a database, I may get some results, and then I pass that to the model, and then the model says, let me think about this. I actually need something else. Then it says, okay, I need to get information from another database. So, it goes, gets information. Now, the model is essentially creating, or the agent is essentially creating kind of this prompt on the fly that is helping the model think through the problem, and at the end of it, it's almost like the model has sufficient information to make the final kind of decision, and it says, you know, it has, I have all the information I need, and here's the final answer. Now, this process of having the model think on what information is presented and kind of doing it in a recursive manner, I think that is truly transformative. Also, this is the concept behind it, which you may have heard of the popular thing, which is auto GPT, where essentially you created this agent, and then the agent kind of was able to quickly kind of understand a broader goal, and then kind of create subtasks, and kind of keep going and solving them on its own. And particularly when you are able to combine things like do web search as one of the tools in its arsenal, I think there are a lot of interesting kind of avenues that open up. But I just wanted to kind of talk about this in the context of retrieval, augmented generation, and what are all the cool things that one can do simply by kind of not even actually customizing the model by tinkering with its model weights. Now, the other kind of side of the story is that in context learning, you need larger models which are capable with long context lengths, and you just do it on the fly without any fine tuning. But on the other end of the spectrum, what we hear from companies is I have specific set of use cases, and I am very cost sensitive. I would rather take a more kind of appropriate model, which gives me the right combination of accuracy, latency, and cost. And I can just fine tune that for my particular use case using my proprietary data. Now, that approach is also a very fine approach because you're not then constantly dealing with a large model, and particularly in situations where latency is a factor, when latency and cost are factors, you may just want to go down that route with fine tuning. Again, in fine tuning as well, there is the full blown fine tuning, and then there are things like lightweight kind of parameter efficient fine tuning and things like LoRa that have emerged in the last few months. You mentioned cost in there. Are you suggesting that the fine tuning approach is more cost effective than in context learning? So here's the thing, right? If you want to do pure in context learning, two things have to happen. One is you need a model with the longer context lengths, because let's just take that example of kind of auto GPT, where you are going to probably pass a lot of kind of API expects and recursively it's going to keep adding context, you'd probably long context links. The second thing is you need is a well trained model, which can be large in size and typically larger in size means that the hosting can be expensive because it needs more expensive machines under the hood to do inference. So because of these two factors, like again, they tend to make inference more costlier, like the longer context length also requires kind of it adds to the hardware requirements in terms of hosting. So as a result, when someone wants to do that, those can be more expensive compared to when I just want to do fine tuning for a particular use case, or using a smaller model compared to the former. Okay. The challenge that I think this is again, there is no kind of we are still in the jury is out. I think for some use cases, I think we are seeing where customers are just going to rely upon this paradigm of larger models in context learning and some use cases where customers are kind of sensitive about certain kind of requirements, particularly around latency and cost, they might just skew in terms of fine tuning. It's hard to tell right now which approach is the right one for a customer without actually understanding the specific details in their use case. So we've talked through a bunch of kind of these steps that are required to put these language models to good use. You've not talked much about the kind of production side of things. Like when we talk about traditional models, like there's this whole, yes, there's training and all that, but then there's all of the inference considerations and evaluation and there's a whole set of considerations on that side of things. How much of that stuff do you see folks struggling with on the LLM side? You're absolutely right. There has been this entire ecosystem around ML ops. I think I do think that some of that ecosystem around ML ops is still relevant in this generative AI space or the foundation model world. Think about something like as companies fine tune different models, they're experimenting essentially, and they're experimenting to see which of these fine tune models ultimately I want to deploy in production. So they may want to actually do some experimentation and see kind of for my use case, is this model, is model A delivering the right results or is it model C? And for that, particularly our service like Bedrock integrates deeply with SageMaker experiments and it allows customers to do precisely that kind of A-B testing and see kind of which of the ultimately the fine tune model they should deploy in production. And of course there are many other things, things like pipelines, right? Where as a company, I may just want to fine tune my model on a periodic basis as soon as I have x amount of new data. I may want to create some recurring things as part of my workflow. So I do think, again, to answer kind of your question, I think on this, there are existing tools, but I also do think that we have to reinvent some of this tooling for the generative AI world because things like prompts, these are our historical way of dealing with systems was more structured than the generative AI world. It's all in terms of kind of this prompt and it's kind of unstructured. It just kind of, while it's very powerful, it also creates a unique set of things because not everything can be classified like in a binary way as it's awesome or it's terrible. It always requires some human judgment in terms of kind of its output because let's just take an example of a summary, model generated summary. And if you ask two different people, what do they think? They may both say kind of, hey, it's summary is correct, but I like one versus the other. And that's why I'm going to pick a different one. It can be subjective. So I think we have to figure out kind of how to tackle that. I think also on the inference side, there are other challenges like hallucinations. A more common kind of thing with large language models is that while they can be, they can give a very compelling human-like response. They sometimes also tend to make up answers and completely inaccurate answers, right? Apparently there was this lawyer who was trying to use a large language model and the large language model gave responses, citing cases that were completely made up. Now, I think as a customer, as you are trying to rely on this, I think it's very important that customers are aware of these issues. And also as someone like us who is working on these technologies, we are taking various steps to ensure that we train our models in a way that they almost learn to say, I don't know. Right? So that is number one. Also techniques like retrieval, augmented generation or RAG, they actually help steer kind of reduce the hallucination because essentially you're giving them context as part of the prompt and essentially guiding the model to say your scope of your answer should be mostly limited to this. So stick to the context that's given to you. Stick to the script. Exactly. That's actually a good one. I've never actually used it. I might borrow this. But again, the challenge here is if you purely stick to the script, then the distinction between the generative model and plain search disappears. You cannot have the cake and eat it too almost. So you still need some amount of creativity left in the model, but at the same time, you want to make sure that they are not kind of BSE. So that's where kind of, I think the, the science will evolve in the next few weeks and months. Yeah. You mentioned bedrock in the context of explaining some of the operational aspects of fielding LLM based solutions. Like talk about bedrock. Like what, what is bedrock? What role does it play? What are some of the key capabilities? Sure. So bedrock is the easiest way for a developer in any company to build generative AI based apps. It does not require this developer to understand any nuances of machine learning and they don't even need to deal with any underlying infrastructure. It's a fully managed service that allows these developers to build any compelling apps using a range of different foundation models. So just in terms of kind of the key points I want to highlight, the first one is around choice. With bedrock, obviously we have embarked upon building our own set of foundation models, which we call as Titan. But in addition to offering the Titan models, we have also partnered with a range of innovative startups in this space and specifically Anthropic AI 21 and Stability AI. So Anthropic is a famous name in this space. You know, I mentioned them a few times already in this podcast. And then AI 21 is an innovative Israeli startup and Stability AI is the company behind stable diffusion. So we have an amazing set of partners on bedrock. And then the advantage with this approach is customers get choice. They are not just stuck to models kind of from one particular provider. The second is around, again, I think we mentioned this a few times in this podcast where this large language models tend to be, you know, they are compute hungry and they are data hungry. Essentially, which means that they can be expensive to build and they can be expensive to run, particularly with things like longer context lengths and whatnot, and their larger size. So what has happened in the past few years is Amazon has invested a lot in its custom silicon efforts. And specifically, we have built a chip called Tranium, which is a machine learning chip optimized for training and Inferentia, which is our machine learning chip optimized for inference. Now, these chips give us anywhere from 40 to 50% price performance advantage compared to similar EC2 instances. And essentially, as with all things Amazon, and anytime we have a materially better cost structure, we want to pass some of those benefits to our customers. So that is kind of where we think we can help customers use or deploy generative AI based apps at scale when they use Bedrock. And then the next thing is around customization and doing it in a secure manner. So obviously, we discussed a lot about customization today. So first and foremost, Bedrock is a service that is default by default opt out. So we don't store any customer data for improving the broader models, which I think is a very, very important point given some of the security concerns that we've heard from a lot of customers. In fact, you may have even heard like some kind of cases where people used a particular application and some of their data, proprietary data was leaked. And that's kind of a very scary prospect for a lot of CIOs out there. So we are opt out by default. Second is anytime a customer customizes the model, essentially what happens is we create a copy of the base model and we create a customized copy for them. And that model is private for that particular enterprise customer. And any data flowing through that model is only their data and no other customer data flows through this. Also, customers can use Bedrock with the kind of all the familiar security constructs that they are accustomed to. So things like VPC private link and using their own VPC, essentially ensuring that no data actually goes to the public internet, which is a pretty key point for a lot of enterprise customers. And the last one is around tooling. So we talked about integration with SageMaker experiments for things like MLOps, particularly for comparing which model do I ultimately need to deploy in production. But also a lot of the things that are evolving is like things like rag, things like being able to create agents, like a lot of the tooling, I think that tooling is going to be important. And I think we want to focus on making some of the tooling available to a lot of customers. In terms of a developer experience, how do developers interact with it? And maybe another way to ask this is prior to Bedrock existing, a developer could use something like SageMaker Jumpstart or Model Catalog or something to build around third party models. Like is Bedrock the same or is it different from an experience perspective? Yeah, so Bedrock is different. So Bedrock is a serverless API. So a developer does not need to understand concepts like infrastructure, instances, endpoints, network topology. All they do is they get a simple serverless API, and they can just select a model and they get to use it right away. They don't have to do any setup. They pay for what they use in terms of tokens. Again, some of the things like being able to swap a model. I think that's super easy. You just basically reference the model type you want. And even things like customization, you essentially point your proprietary data sitting in a three bucket. We do the customization or the actual training under the hood, and a customized resource is ready for you. You essentially reference that resource or an Amazon resource name as part of your API, and you're good to go. So the experience is very simple. From like enterprise governance perspective, do you get questions like that? You mentioned VPCs and some of the security considerations. On the MLops side of things, we talk a lot about like model life cycles and that kind of thing. Are we at the point where we know how to life cycle these customized LLM models that we're building into applications? That's a good question. So I think again, as we think about pros and cons as a company, whether I want to use models in context without actual fine tuning and changing the weights of the model, the advantages is because I get to use the latest and the greatest model. And then I fine tune it. Of course, I get the benefits of being able to do it in a cost effective manner. But the one challenge could be that, hey, because anytime you fine tune a model or customize a model by changing the weights of the model, and a copy of the base model is created in your own model. So any upgrades that happen to the base model in the future will not directly propagate to this fine tune model. So let's say that we have a model version one and tomorrow we have a model version two after a few months, a customer who has fine tuned it will probably have to refine tune their model using their data, either the new data or the prior data that they used. So there is some figuring out that has to be done in terms of how customers keep up. And maybe we will have more pipelines around this. That's where the concept of pipelines comes into picture, where you have almost automated things. When you have a new model version come up, you can quickly refine tune and using the latest data and create your proprietary model. But again, I think Bedrock is currently in preview and we are rapidly iterating to add a lot of certifications around compliance and whatnot. And that should kind of, you know, a lot of enterprise customers ask for that. So we will be taking care of that part. But some of these questions you're asking, I think those are pretty pertinent ones. And I think our thinking on that is evolving as we speak. Awesome. Where can folks go to learn more about Bedrock and the topics that we've talked about? So the first and foremost, a few weeks ago, Swami, who's the head of machine learning data and analytics at Amazon, he published a generative AI based blog. I would request the listeners to look at it. And we talk a lot about a lot of different things, including Bedrock, Code Whisperer, and some of our investments in custom silicon. In addition to that, of course, listeners can go to aws.amazon.com slash bedrock to learn more about the specific service. Awesome. And we'll drop links to both of those resources in the show notes page. And there's a bunch of other really interesting kind of technical blogs like how to do retrieval, augmented generate. I think there's like two or three different takes using different tech stacks and a bunch of other folks. I think the developer evangelist team has been busy there and cranking these things up. And we'll put a link to those as well. So it's been great chatting any thoughts on what we should be kind of looking forward to or you know, what you're most excited about looking forward? I think the pace of innovation in this space has been staggering. I mean, I don't think either you or I have seen anything in our lifetimes. And I'm really happy to be part of this transformation. Really can't wait to see what customers will build with Bedrock. And then just in general, in this space, we're in the golden age. Really, really excited for the times ahead. Absolutely. All right, Ato, thanks so much. It was great chatting with you. Thanks. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimbleai.com. Of course, if you like what you hear on the podcast, please subscribe, rate and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "Bedrock is a fully managed service that allows developers to build generative AI-based apps without needing to understand machine learning or deal with infrastructure. It offers a range of foundation models, including the Titan models developed by Amazon and partnerships with other innovative startups in the space. Bedrock also leverages Amazon's custom silicon efforts, such as the Tranium and Inferentia chips, to provide better cost performance for running large language models. The service enables customization of the models using proprietary data in a secure manner, ensuring data privacy and compliance. It integrates with SageMaker experiments for ML ops and provides a simple developer experience with a serverless API. The challenge of life-cycling customized LLM models is still being addressed as the field evolves. There is a rapid pace of innovation in the space, and Bedrock is excited to see what customers will build with their platform.", "podcast_guest": "Atul Dayo", "podcast_highlights": "Some key moments in the podcast include:\n\n- The introduction of Atul Dayo, the general manager of Amazon Bedrock.\n- Discussion about the paradigm shift in machine learning and the challenge of dealing with the language in the generative AI landscape.\n- The explanation of foundation models and how they differ from traditional machine learning models.\n- The concept of in-context learning and its benefits.\n- The challenges and considerations of using generative AI models at scale.\n- The introduction of Bedrock, a fully managed service for building generative AI-based apps.\n- The role of customization and security in using Bedrock.\n- The importance of tooling and integration with other services like SageMaker experiments.\n- The challenges of production and operationalizing LLM models.\n- The future of generative AI and the excitement around innovation in this space."}